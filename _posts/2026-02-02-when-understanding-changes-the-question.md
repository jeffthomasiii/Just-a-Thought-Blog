---
layout: post
title: "When Understanding Changes the Question"
subtitle: ""
description: "A personal reflection on how AI tools are reshaping our relationship with understanding, trust, and discernment."
date: 2026-02-02
author: Jeff Thomas III
categories:
  - technology
  - reflection
tags:
  - AI
  - discernment
  - understanding
  - NotebookLM
  - wisdom
excerpt: "AI hasnâ€™t made me smarterâ€”itâ€™s made me more aware of what I donâ€™t understand, and how often Iâ€™ve been willing to move forward anyway."
image: /img/posts/when-understanding-changes-the-question.jpg
background: /img/posts/bg-when-understanding-changes-the-question.jpg
---

I remember listening to an AI-generated audio overview of one of my own blog posts and stopping halfway through.

My first thought was, *Thatâ€™s exactly what I was trying to say.*  
My second was, *Thatâ€™s not what I meantâ€¦ but itâ€™s an interesting take.*

That moment stayed with me.  
Not because it felt impressive, but because it felt revealing.

For a long time, Iâ€™ve accepted a quiet reality most of us donâ€™t like to admit out loud: there are things we agree to, approve, or move past without fully understanding them.

Not because weâ€™re careless.  
But because the material itself feels impenetrable.

Long documents.  
Dense language.  
â€œStandard terms.â€

At some point, trust replaces comprehension.

Lately, that dynamic has cracked open for me.

Iâ€™ve been spending time with AI tools that are remarkably good at research, parsing massive documents, summarizing, and, more importantly, *contextualizing* information. Not just telling me what something says but helping me see how it fits together. Where it contradicts itself. Where it quietly assumes things I may not actually agree with.

Itâ€™s made me wonder how many decisions in modern life are built on borrowed confidence.

Take contracts, for example.

Imagine uploading a long agreement and asking a few clarifying questions:  
Where is the risk concentrated?  
Which sections are intentionally vague?  
What assumptions does this language make about me?  
Where would someone with a different risk tolerance pause?

Thatâ€™s a very different posture than, *I have no idea what this says, but they tell me itâ€™s standard language.*

Someone shared a story with me recently that stuck.

They uploaded a massive document into several different AI tools and thought they had a solid grasp of it. Then they ran the same material through another tool designed to stay grounded in the original source.

Their response was blunt: *â€œBoy was I wrong.â€*

It surfaced discrepancies.  
Missed nuances.  
Contextual gaps the others glossed over.

That experience mirrors my own, just in smaller ways.

It doesnâ€™t feel like being replaced.  
It feels like being *reflected*.

Sometimes accurately.  
Sometimes imperfectly.  
Always revealing something about how ideas land once they leave my head.

Iâ€™ve even used AI to generate infographics using nothing but a single blog post as the source. No added ideas. No external framing. Just structure pulled from what was already there.

I donâ€™t love everything about the result. Some choices feel off. Some emphasis isnâ€™t how I would have framed it. But seeing my own words reorganized visually, without outside input, was still impressive.

Not because it was perfect, but because it revealed shape.

<!-- ğŸ“Œ IMAGE INSERT POINT: Place infographic directly below this line -->

![AI-generated infographic derived from a single source document](/img/posts/ai-infographic-single-source.jpg)

*Caption: An AI-generated infographic created using only one source document, imperfect, but revealing how structure emerges when ideas are reorganized.*

At the same time, Iâ€™m watching people use these tools in increasingly creative, and sometimes obviousâ€”ways. LinkedIn articles generated for reach. Content pipelines optimized for traffic.

AI doesnâ€™t just surface information.  
It surfaces intention.

And somewhere along the way, the question around AI quietly shifted.

It used to be, *Are you using AI?*  
Now itâ€™s more often, *How are you using it?*  
And maybe even, *Do you trust it?*

That last question matters more than we think.

Because trust isnâ€™t the same as usefulness.  
And assistance isnâ€™t the same as delegation.

These tools are incredibly good at helping us *see*.  
They are poor substitutes for judgment.

They donâ€™t carry values.  
They donâ€™t own consequences.  
They donâ€™t live with outcomes.

They just surface whatâ€™s there.

What we do with it still says something about us.

People are already using AI in churches and ministries too, for writing, organization, planning, communication. I have strong thoughts about that space, but they deserve a slower, more careful conversation.

That reflection is coming later, in a series Iâ€™m calling *Faith, Wisdom, and the Machine*.

For now, Iâ€™m sitting with something simpler.

AI hasnâ€™t made me smarter.  
Itâ€™s made me more aware of what I donâ€™t understand, and how often Iâ€™ve been willing to move forward anyway.

Maybe the real shift isnâ€™t technological at all.  
Maybe itâ€™s relational.

Between us and information.  
Between confidence and clarity.  
Between speed and discernment.

When understanding changes, the questions we ask change too.

â€¦just a thought.

<!--stackedit_data:
eyJoaXN0b3J5IjpbLTExMjU2NzAxNjhdfQ==
-->